{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsZ5gDlaYget"
   },
   "source": [
    "##üöÄ Finetuning MLP on Fashion MNIST dataset (with [W&B experiment](https://docs.wandb.ai/quickstart) tracking)\n",
    "\n",
    "üß† Finetuner lets you tune the weights of any deep neural network for better embeddings on search tasks. It is a powerful human-in-the-loop deep learning tool to level up the performance of pre-trained models for domain-specific applications.\n",
    "\n",
    "üí° In this example, we will finetune a simple MLP model on the fashion MNIST dataset and use the Weights & Biases experiments to track and monitor the training process. \n",
    "\n",
    "üìå The purpose of this example is to show all things possible with finetuner and give you a glimpse of its features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fY5t2L8aq6l"
   },
   "source": [
    "### ‚è∞ Installing & Importing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlpmYMPFa04U"
   },
   "source": [
    "We will start this tutorial by installing the necessary ***pip*** dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5Nre8KZCCMEa"
   },
   "outputs": [],
   "source": [
    "!pip install torchvision\n",
    "!pip install finetuner==0.4.0\n",
    "!pip install docarray==0.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_kH00dva45f"
   },
   "source": [
    "We will import the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Yk5SPmInCSZF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from docarray import Document\n",
    "import numpy as np\n",
    "from finetuner.toydata import generate_fashion\n",
    "from finetuner.tuner.callback import WandBLogger\n",
    "from finetuner.tuner.pytorch import PytorchTuner\n",
    "from finetuner.tuner.pytorch.losses import TripletLoss\n",
    "from finetuner.tuner.pytorch.miner import TripletEasyHardMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65IOt4YYdakV"
   },
   "source": [
    "### üîë Authenticate W&B Dashboard\n",
    "\n",
    "Before proceeding further, we need to install the W&B library and authenticate the account via login to track the experiments on the UI dashboard. \n",
    "\n",
    "You can find more details [here](https://docs.wandb.ai/quickstart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaGOgABol9Wt"
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fiHMwSkeGpJ"
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XMHgtUEb7Ut"
   },
   "source": [
    "### üî® Data Preprocessing (Using [DocArray](https://docarray.jina.ai/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5eUB0J6bAEh"
   },
   "source": [
    "In this step we will load the training and evaluation(test) data usign the  [`generate_fashion()`](https://finetuner.jina.ai/api/finetuner.toydata/#finetuner.toydata.generate_fashion) helper function, which will produce a [Class Dataset](https://finetuner.jina.ai/basics/datasets/class-dataset/#class-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQYeWInxCSR-"
   },
   "outputs": [],
   "source": [
    "train_data = generate_fashion()\n",
    "eval_data = generate_fashion(is_testset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYx9U5pRcX9p"
   },
   "source": [
    "Now, we will preprocess the data by adding some noise to it for the model to be trained properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNTYFwfHbwJR"
   },
   "outputs": [],
   "source": [
    "def preprocess_fn(doc: Document) -> np.ndarray:\n",
    "    \"\"\"Add some noise to the image\"\"\"\n",
    "    new_image = doc.tensor + np.random.normal(scale=0.01, size=doc.tensor.shape)\n",
    "    return new_image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-tNJnfrdVxw"
   },
   "source": [
    "### ‚ö° Create Model \n",
    "\n",
    "We will create a simple MLP [embedding model](https://finetuner.jina.ai/basics/glossary/#term-Embedding-model) using the Pytorch framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udjQqOqpdFjF"
   },
   "outputs": [],
   "source": [
    "# create a MLP model\n",
    "embed_model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(in_features=28 * 28, out_features=128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=128, out_features=32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd0rvPcxiIOq"
   },
   "source": [
    "We will create a function to use `Adam` optimizer and dynamic scheduler for better learning rate and model training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2eLeAw4gYUj"
   },
   "outputs": [],
   "source": [
    "# Function to configure learning rate optimizer and scheduler\n",
    "def configure_optimizer(model):\n",
    "    optimizer = Adam(model.parameters(), lr=5e-4)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[10, 20], gamma=0.5)\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhqY4JJggf7J"
   },
   "outputs": [],
   "source": [
    "# Creating the object for loss function and W&B callback\n",
    "loss = TripletLoss(\n",
    "    miner=TripletEasyHardMiner(pos_strategy='easy', neg_strategy='semihard')\n",
    ")\n",
    "logger_callback = WandBLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snacMK0mgk9G"
   },
   "source": [
    "### ‚è≥ Model Finetuning\n",
    "\n",
    "We will create the `PytorchTuner` object and specify the training configuration. We will use the following configuration:\n",
    "\n",
    "\n",
    "* `Triplet loss` function using hard miner with the easy positive and semi-hard negative strategy.\n",
    "* `Adam` optimizer with an initial learning rate of `0.0005`, which will be halved every `30` epochs\n",
    "*  Tracking the experiment on Weights and Biases using `WandBLogger` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgAODRfOgqDt"
   },
   "outputs": [],
   "source": [
    "# Creating a tuner object for Pytorch model\n",
    "tuner = PytorchTuner(\n",
    "    embed_model,\n",
    "    loss=loss,\n",
    "    configure_optimizer=configure_optimizer,\n",
    "    scheduler_step='epoch',\n",
    "    callbacks=[logger_callback],\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9bOAwUWhJmf"
   },
   "outputs": [],
   "source": [
    "# Fitting the tuner \n",
    "tuner.fit(\n",
    "    train_data, eval_data, preprocess_fn=preprocess_fn, epochs=40, num_items_per_class=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHjsJAIMmdOJ"
   },
   "source": [
    "‚ú® You can monitor the training process by logging into your W&B account and watch the live updates there. \n",
    "\n",
    "Here‚Äôs an example of what you may see üëâ\n",
    "\n",
    "![](https://finetuner.jina.ai/_images/wandb.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3Rfyz-c5zEh"
   },
   "source": [
    "### ‚úà Next Steps\n",
    "\n",
    "Replace the pre-trained model in your existing application with the finetuned model to increase accuracy and performance.\n",
    "\n",
    "Read the finetuner [documentation](https://finetuner.jina.ai/) to understand its functioning in detail.\n",
    "\n",
    "Check out the existing blog posts to understand the application of finetuner for real-world projects: \n",
    "\n",
    "* [Improving Image Similarity Detection using finetuner](https://colab.research.google.com/github/Shubhamsaboo/jina_finetuner_notebooks/blob/master/Finetuner_ImageSimilarityDetection.ipynb?authuser=4#scrollTo=3FA9KO7zzbVD) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Finetuner_W&B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
