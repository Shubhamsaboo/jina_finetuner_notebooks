{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##üöÄ Finetuning MLP on Fashion MNIST dataset (with W&B experiment tracking)\n",
        "\n",
        "üí° In this example, we will finetune a simple MLP model on the fashion MNIST dataset and use the Weights & Biases experiments to track and monitior the training process. \n",
        "\n",
        "üìå The purpose of this example is to show all things possible with finetuner and give you a glimpse of its features.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EsZ5gDlaYget"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚è∞ Installing & Importing Dependencies"
      ],
      "metadata": {
        "id": "6fY5t2L8aq6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start this tutorial by installing the necessary ***pip*** dependencies."
      ],
      "metadata": {
        "id": "HlpmYMPFa04U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5Nre8KZCCMEa"
      },
      "outputs": [],
      "source": [
        "!pip install finetuner\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will import the necessary dependencies."
      ],
      "metadata": {
        "id": "5_kH00dva45f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yk5SPmInCSZF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "from finetuner.toydata import generate_fashion\n",
        "from finetuner.tuner.callback import WandBLogger\n",
        "from finetuner.tuner.pytorch import PytorchTuner\n",
        "from finetuner.tuner.pytorch.losses import TripletLoss\n",
        "from finetuner.tuner.pytorch.miner import TripletEasyHardMiner"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîë Authenticate W&B Dashboard\n",
        "\n",
        "Before proceeding further, we need to install the W&B library and authenticate the account via login to track the expriements on the UI dashboard. \n",
        "\n",
        "You can find more details [here](https://docs.wandb.ai/quickstart)."
      ],
      "metadata": {
        "id": "65IOt4YYdakV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "qaGOgABol9Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "5fiHMwSkeGpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üî® Data Preprocessing (Using [DocArray](https://docarray.jina.ai/))"
      ],
      "metadata": {
        "id": "6XMHgtUEb7Ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step we will load the training and evaluation(test) data usign the  [`generate_fashion()`](https://finetuner.jina.ai/api/finetuner.toydata/#finetuner.toydata.generate_fashion) helper function, which will produce a [Class Dataset](https://finetuner.jina.ai/basics/datasets/class-dataset/#class-dataset)."
      ],
      "metadata": {
        "id": "U5eUB0J6bAEh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQYeWInxCSR-"
      },
      "outputs": [],
      "source": [
        "train_data = generate_fashion()\n",
        "eval_data = generate_fashion(is_testset=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will preprocess the data by adding some noise to it for the model to be trained properly."
      ],
      "metadata": {
        "id": "UYx9U5pRcX9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_fn(doc: Document) -> np.ndarray:\n",
        "    \"\"\"Add some noise to the image\"\"\"\n",
        "    new_image = doc.blob + np.random.normal(scale=0.01, size=doc.blob.shape)\n",
        "    return new_image.astype(np.float32)"
      ],
      "metadata": {
        "id": "tNTYFwfHbwJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° Create Model \n",
        "\n",
        "We will create a simple MLP [embedding model](https://finetuner.jina.ai/basics/glossary/#term-Embedding-model) using the Pytorch framework. "
      ],
      "metadata": {
        "id": "9-tNJnfrdVxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a MLP model\n",
        "embed_model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(in_features=28 * 28, out_features=128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features=128, out_features=32),\n",
        ")"
      ],
      "metadata": {
        "id": "udjQqOqpdFjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a fucntion to use `Adam` optimizer and dynamic scheduler for better learning rate and model training results."
      ],
      "metadata": {
        "id": "Hd0rvPcxiIOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to configure learning rate optimizer and scheduler\n",
        "def configure_optimizer(model):\n",
        "    optimizer = Adam(model.parameters(), lr=5e-4)\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[10, 20], gamma=0.5)\n",
        "\n",
        "    return optimizer, scheduler"
      ],
      "metadata": {
        "id": "q2eLeAw4gYUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the object for loss function and W&B callback\n",
        "loss = TripletLoss(\n",
        "    miner=TripletEasyHardMiner(pos_strategy='easy', neg_strategy='semihard')\n",
        ")\n",
        "logger_callback = WandBLogger()"
      ],
      "metadata": {
        "id": "bhqY4JJggf7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚è≥ Model Finetuning\n",
        "\n",
        "We will create the `PytorchTuner` object and specify the training configuration. We will use the following configuration:\n",
        "\n",
        "\n",
        "* `Triplet loss` function using hard miner with the easy positive and semi-hard negative strategy.\n",
        "* `Adam` optimizer with initial learning rate of `0.0005`, which will be halved every `30` epochs\n",
        "*  Tracking the experiement on Weights and Biases using `WandBLogger` callback."
      ],
      "metadata": {
        "id": "snacMK0mgk9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a tuner object for Pytorch model\n",
        "tuner = PytorchTuner(\n",
        "    embed_model,\n",
        "    loss=loss,\n",
        "    configure_optimizer=configure_optimizer,\n",
        "    scheduler_step='epoch',\n",
        "    callbacks=[logger_callback],\n",
        "    device='cpu',\n",
        ")"
      ],
      "metadata": {
        "id": "BgAODRfOgqDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the tuner \n",
        "tuner.fit(\n",
        "    train_data, eval_data, preprocess_fn=preprocess_fn, epochs=40, num_items_per_class=32\n",
        ")"
      ],
      "metadata": {
        "id": "S9bOAwUWhJmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ú® You can monitor the training process by logging into your W&B account and watch the live updates there. \n",
        "\n",
        "Here‚Äôs an example of what you may see üëâ\n",
        "\n",
        "![](https://finetuner.jina.ai/_images/wandb.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "nHjsJAIMmdOJ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Finetuner_W&B.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}