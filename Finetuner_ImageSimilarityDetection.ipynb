{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetuner_ImageSimilariltyDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Finetuning ResNet for Totally Looks Like Dataset\n",
        "\n",
        "üí° Totally-Looks-Like is a benchmarking dataset for image similarity detection. Today, we have deep learning models that can determine whether two images are similar or not with a certain level of accuracy. \n",
        "\n",
        "‚ùì If you want to deploy these models in the real world, the accuracy needs to be at par with human perception of image similarity. But how would you increase the accuracy of pre-trained deep learning models? üëâ That's where Finetuner comes in! \n",
        "\n",
        "üß† Finetuner lets you tune the weights of any deep neural network for better embeddings on search tasks.\n",
        "\n",
        "üé® In this example, we will finetune ResNet50 on [Totally Looks Like dataset](https://sites.google.com/view/totally-looks-like-dataset) for similar image detection and will see how it affects the accuracy of the model."
      ],
      "metadata": {
        "id": "kFgJIbs7t5Yq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚è∞ Installing & Importing Dependencies"
      ],
      "metadata": {
        "id": "QEuqqJrKyOpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start this tutorial by installing the necessary ***pip*** dependencies. "
      ],
      "metadata": {
        "id": "il027E8At5Kw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li6wgGm6ZAsx"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!pip install finetuner\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will import the necessary dependencies."
      ],
      "metadata": {
        "id": "lbS96XxvyYxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "import finetuner as ft\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from finetuner.tuner.pytorch.losses import TripletLoss\n",
        "from finetuner.tuner.pytorch.miner import TripletEasyHardMiner"
      ],
      "metadata": {
        "id": "Qg-HzxyQyNaq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üî® Data Preparation \n",
        "\n",
        "In this step, we will download the data using the ***gdown*** library. \n",
        "\n",
        "We will download the data as two folders -> `left.zip` and `right.zip`\n",
        "\n",
        "Each of them consists of *6016 images* which can be formed into pairs based on the same file name."
      ],
      "metadata": {
        "id": "ntqCjjW5vedt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1jvkbTr_giSP3Ru8OwGNCg6B4PvVbcO34\n",
        "!gdown https://drive.google.com/uc?id=1EzBZUb_mh_Dp_FKD0P4XiYYSd0QBH5zW"
      ],
      "metadata": {
        "id": "RauqShc7ZY9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then unzip the data to be used for further pre-processing."
      ],
      "metadata": {
        "id": "3S2ZGswWwRlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if path.exists(\"/content/left\") & path.exists(\"/content/right\"):\n",
        "  print(\"File directory already exists\")\n",
        "else:\n",
        "  !unzip left.zip\n",
        "  !unzip right.zip"
      ],
      "metadata": {
        "id": "LhRAA1XxZkRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üïπ Data Pre-Processing & Manipulation (Using [DocArray](https://docarray.jina.ai/))\n",
        "\n",
        "We will perform the following pre-processing tasks to prepare the data for model input:\n",
        "\n",
        "1. We will load all images from unzipped `left` and `right` folders and turn them into sorted order as Jina's `DocumentArray`\n",
        "2. After that, we will do a train/test split (Training Data - 80%, Test Data - 20%)"
      ],
      "metadata": {
        "id": "QMDvW5cHwbvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docarray import DocumentArray\n",
        "\n",
        "left_da = DocumentArray.from_files('left/*.jpg')\n",
        "right_da = DocumentArray.from_files('right/*.jpg')\n",
        "# we use 80% for training machine learning model.\n",
        "left_da.sort(key=lambda x: x.uri)\n",
        "right_da.sort(key=lambda x: x.uri)\n",
        "\n",
        "ratio = 0.8\n",
        "train_size = int(ratio * len(left_da))\n",
        "\n",
        "train_da = left_da[:train_size] + right_da[:train_size]"
      ],
      "metadata": {
        "id": "vbyAPwk3Zprz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üíª Preparing the training data (Using [Finetuner](https://finetuner.jina.ai/))\n",
        "\n",
        "After loading data into Jina DocumentArray, we can prepare documents for training using Finetuner that makes the entire process like a breeze. We just have to do the following:\n",
        "\n",
        "1. Assign a label into each Document named `finetuner_label` as its class name.\n",
        "\n",
        "2. Pre-process each document:\n",
        "\n",
        "  *  Load the image from the URI\n",
        "  *  Normalize the image and reshape the image from `H, W, C` to `C, H W` will `C` is the color channel of the image.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KFFkUMIoygzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_label_and_preprocess(doc):\n",
        "    doc.tags['finetuner_label'] = doc.uri.split('/')[1]\n",
        "    return doc.load_uri_to_image_blob().set_image_blob_normalization().set_image_blob_channel_axis(-1, 0)"
      ],
      "metadata": {
        "id": "T-xUdKETakpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_da.apply(assign_label_and_preprocess)"
      ],
      "metadata": {
        "id": "-WSGS8WKaxdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚òÅ Loading the Pre-trained Model\n",
        "\n",
        "We will load the pre-trained ResNet50 model from torchvision. Since we want to learn a better `embedding`, the first thing is to see which layer is suitable for use as an `embedding layer`. \n",
        "\n",
        "You can call `finetuner.display(model, input_size)` to plot the model architecture."
      ],
      "metadata": {
        "id": "vyb_-1_f0pzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = torchvision.models.resnet50(pretrained=True)\n",
        "ft.display(resnet, (3, 224, 224))"
      ],
      "metadata": {
        "id": "WoCi6gpLbIB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model is pre-trained on ImageNet data for classification task, so the output `fc` layer should not be considered as embedding layer. \n",
        "\n",
        "Instead, we can use the pooling layer `adaptiveavgpool2d_173` as the output of the embedding model. This layer generates a `2048` dimensional dense embedding as output.\n",
        "\n",
        "To make the model produce the desired output, you can use the [Tailor](https://finetuner.jina.ai/components/tailor/) component of finetuner. \n"
      ],
      "metadata": {
        "id": "vEM6ZESE2jAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚è≥ Model Finetuning\n",
        "\n",
        "To finetune any pre-trained model üëâ \n",
        "\n",
        "*   Plug in the pre-trained model\n",
        "*   Plug in the training data\n",
        "*   Configure the hyperparameters\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EYD3l7Q-1iS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below combines [Tailor](https://finetuner.jina.ai/components/tailor/) & [Tuner](https://finetuner.jina.ai/components/tuner/) interface for model fine-tuning.\n",
        "\n",
        "We will save the returned embedding model as `tuned_model`, given an input image, at inference time, this model will generate a ***2048-dimension vector*** representation of the image.\n",
        "\n",
        "To undestand the usage of each parameter in detail refer to this [tutorial](https://finetuner.jina.ai/get-started/totally-looks-like/) and the [documentation](https://finetuner.jina.ai/).\n",
        "\n"
      ],
      "metadata": {
        "id": "dHpNaKR83VSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_model = ft.fit(\n",
        "    model=resnet,\n",
        "    train_data=train_da,\n",
        "    epochs=6,\n",
        "    batch_size=128,\n",
        "    loss=TripletLoss(miner=TripletEasyHardMiner(neg_strategy='hard'), margin=0.3), \n",
        "    learning_rate=1e-5,\n",
        "    device='cuda',\n",
        "    to_embedding_model=True,\n",
        "    input_size=(3, 224, 224),\n",
        "    layer_name='adaptiveavgpool2d_173',\n",
        "    num_items_per_class=2,\n",
        "    freeze=['conv2d_1', 'batchnorm2d_2', 'conv2d_5', 'batchnorm2d_6', 'conv2d_8', 'batchnorm2d_9', 'conv2d_11', 'batchnorm2d_12'],\n",
        ")"
      ],
      "metadata": {
        "id": "IF7tsfPmbQgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé Evaluating the embedding quality\n",
        "\n",
        "We will use the ***hit@10*** method to evaluate the quality of finetuned embeddings with the pre-trained embeedings. \n",
        "\n",
        "***hit@10*** means for all the test data, how likely is it for the positive match to be ranked within the top 10 matches with respect to the `query` Document."
      ],
      "metadata": {
        "id": "fpBkQjV94lxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already have the `train_da` ready, now we will perform the same preprocessing on test `DocumentArray`."
      ],
      "metadata": {
        "id": "_rKK3zOK560M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(doc):\n",
        "    return doc.load_uri_to_image_blob().set_image_blob_normalization().set_image_blob_channel_axis(-1, 0)"
      ],
      "metadata": {
        "id": "mVr0XZlZp0Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_left_da = left_da[train_size:]\n",
        "test_right_da = right_da[train_size:]\n",
        "\n",
        "test_left_da.apply(preprocess)\n",
        "test_right_da.apply(preprocess)"
      ],
      "metadata": {
        "id": "r-gR9ChXcCWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the embeddings for the test set using the fine-tuned model."
      ],
      "metadata": {
        "id": "qZt95t8e6_AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use finetuned model to create embeddingsÔºå only test data\n",
        "test_left_da.embed(tuned_model, device='cuda')\n",
        "test_right_da.embed(tuned_model, device='cuda')"
      ],
      "metadata": {
        "id": "NyGDsk-acm3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then match `test_left_da` against `test_right_da`.\n",
        "\n",
        "You can consider `test_left_da` as user queries, while `test_right_da` is our indexed document collection. \n",
        "\n",
        "For each `test_left_da`, match function will find ***top-10*** nearest embeddings in `test_right_da`. And we evaluate result with ***hit@10***"
      ],
      "metadata": {
        "id": "owlUsxzp7ewu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hit_rate(da, topk=1):\n",
        "    hit = 0\n",
        "    for d in da:\n",
        "        for m in d.matches[:topk]:\n",
        "            if d.uri.split('/')[-1] == m.uri.split('/')[-1]:\n",
        "                hit += 1\n",
        "    return hit/len(da)"
      ],
      "metadata": {
        "id": "3Dkwv67ge1O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_left_da.match(test_right_da, limit=10)"
      ],
      "metadata": {
        "id": "36FhcI7deknP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(1, 11):\n",
        "    print(f'hit@{k}:  finetuned: {hit_rate(test_left_da, k):.3f}')"
      ],
      "metadata": {
        "id": "pqv_ABCbfBh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° Comparision with pre-trained model\n",
        "\n",
        "In this section, we will load the pre-trained model and evaluate its embedding on the test data with finetuned model using the ***hit@10*** method.\n",
        "\n",
        "In the first step, we will chop off the last classification layer and use the model as feature extractor (for creating embeddings)."
      ],
      "metadata": {
        "id": "uEJuyZpLhYB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_pretrained = torchvision.models.resnet50(pretrained=True)\n",
        "resnet_pretrained.fc = nn.Identity()"
      ],
      "metadata": {
        "id": "RN94v5WahhFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and pre-process the test data\n",
        "test_left_da_pretrained = left_da[train_size:]\n",
        "test_right_da_pretrained = right_da[train_size:]\n",
        "\n",
        "test_left_da_pretrained.apply(preprocess)\n",
        "test_right_da_pretrained.apply(preprocess)"
      ],
      "metadata": {
        "id": "hiQByy7DpP9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the pre-trained model to create embeddings only test data\n",
        "test_left_da_pretrained.embed(resnet_pretrained, device='cuda')\n",
        "test_right_da_pretrained.embed(resnet_pretrained, device='cuda')"
      ],
      "metadata": {
        "id": "8yjhi4hohqLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_left_da_pretrained.match(test_right_da_pretrained, limit=10)"
      ],
      "metadata": {
        "id": "ug92M8UFixti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(1, 11):\n",
        "    print(f'hit@{k}:  pre-trained: {hit_rate(test_left_da_pretrained, k):.3f}')"
      ],
      "metadata": {
        "id": "sVMxb4iQi1Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ú® Conclusion\n",
        "Here, you can clearly see the difference between hit rate of fine-tuned and pre-trained model. The finetuned model performs much better in terms of finding the right match. \n",
        "\n",
        "Let‚Äôs look at some results from finetuned and pre-trained model side-by-side. It can clearly observed that finetuned model do a better job at finding the similar images.\n",
        "\n",
        "![](https://finetuner.jina.ai/_images/result-final1.png)\n"
      ],
      "metadata": {
        "id": "Yq2eu-o3EWnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úà Next Steps\n",
        "\n",
        "Read the finetuner [tutorials](https://finetuner.jina.ai/get-started/totally-looks-like/) and [documentation](https://finetuner.jina.ai/) to understand its functioning in detail. "
      ],
      "metadata": {
        "id": "3FA9KO7zzbVD"
      }
    }
  ]
}